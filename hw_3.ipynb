{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import collections\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#constants\n",
    "DATA_FOLDER = \"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_FOLDER, \"WarAndPeace.txt\"), \"r\") as file:\n",
    "    war_and_peace = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = re.compile(r\"[^а-я ]+\")\n",
    "\n",
    "def preprocess_text(text, reg = reg):\n",
    "    text = text.lower()\n",
    "    return re.sub(reg, \"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_data = preprocess_text(war_and_peace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Реализуйте базовый частотный метод по Шерлоку Холмсу:\n",
    "    - подсчитайте частоты букв по корпусам (пунктуацию и капитализацию можно просто опустить, а вот пробелы лучше оставить);\n",
    "    - возьмите какие-нибудь тестовые тексты (нужно взять по меньшей мере 2-3 предложения, иначе вряд ли сработает), зашифруйте их посредством случайной перестановки символов;\n",
    "    - расшифруйте их таким частотным методом.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequncy_of_n_gram(text, n_gram = 1):\n",
    "    length = len(text)\n",
    "    counter = collections.Counter()\n",
    "    for i in range(len(text)):\n",
    "        counter[text[i:i + n_gram]] += 1\n",
    "    normalized_dict = dict({k:v / length for k,v in sorted(counter.items(), key=lambda item: item[1], reverse=True)}) \n",
    "    return normalized_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Russian characters frequency:\n",
      "\n",
      " {' ': 0.1704868147910879, 'о': 0.09437233970469568, 'а': 0.0696204286039879, 'е': 0.0654779137740928, 'и': 0.055189385306238105, 'н': 0.05408214807103566, 'т': 0.047152290548906316, 'с': 0.04331622941832316, 'л': 0.042005716362471586, 'в': 0.03822817402874197, 'р': 0.037837022070826225, 'к': 0.02976450804171466, 'д': 0.025235461158918573, 'м': 0.02454709531172039, 'у': 0.02379867069933042, 'п': 0.02132394157976112, 'я': 0.019214184956357298, 'г': 0.017212226116631042, 'ь': 0.016166587614958636, 'ы': 0.015758496005322132, 'з': 0.014786775983885772, 'б': 0.014337105229116489, 'ч': 0.011317227317806346, 'й': 0.009563203380538496, 'ж': 0.008408227126850272, 'ш': 0.007838438841697415, 'х': 0.007083854355954442, 'ю': 0.005382189342187125, 'ц': 0.003355591009048854, 'э': 0.002508608423010823, 'щ': 0.002331512064111962, 'ф': 0.0018618217209454173, 'ъ': 0.00043581103972502327}\n"
     ]
    }
   ],
   "source": [
    "ru_characters_frequency = get_frequncy_of_n_gram(text = ru_data)\n",
    "print(f\"Russian characters frequency:\\n\\n {ru_characters_frequency}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 'Ах, не говорите мне про Австрию! ' + \\\n",
    "                        'Я ничего не понимаю, может быть, но Австрия никогда не хотела и не хочет войны. Она предает нас. Россия одна должна быть спасительницей Европы. ' + \\\n",
    "                        'Наш благодетель знает свое высокое призвание и будет верен ему. Вот одно, во что я верю. Нашему доброму и чудному государю предстоит величайшая роль в мире, ' + \\\n",
    "                        'и он так добродетелен и хорош, что Бог не оставит его, и он исполнит свое призвание задавить гидру революции, которая теперь еще ужаснее в лице этого убийцы и злодея. '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ах не говорите мне про австрию я ничего не понимаю может быть но австрия никогда не хотела и не хочет войны она предает нас россия одна должна быть спасительницей европы наш благодетель знает свое высокое призвание и будет верен ему вот одно во что я верю нашему доброму и чудному государю предстоит величайшая роль в мире и он так добродетелен и хорош что бог не оставит его и он исполнит свое призвание задавить гидру революции которая теперь еще ужаснее в лице этого убийцы и злодея \n"
     ]
    }
   ],
   "source": [
    "test = preprocess_text(test)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoding_dict(frequencies):\n",
    "    keys = list(frequencies.keys())\n",
    "    shuffled_keys = keys.copy()\n",
    "    np.random.shuffle(shuffled_keys)\n",
    "    encoding_dict = dict(zip(keys, shuffled_keys))\n",
    "    return encoding_dict\n",
    "\n",
    "    \n",
    "def get_decoding_dict(encoded_frequncies, frequencies):   \n",
    "    decoding_dict = dict(list(zip(encoded_frequncies.keys(), frequencies.keys())))\n",
    "    return decoding_dict\n",
    "\n",
    "\n",
    "def calculate_accuracy(text1, text2):\n",
    "    counter = 0\n",
    "    for t1, t2 in zip(text1, text2):\n",
    "        if t1 == t2:\n",
    "            counter += 1\n",
    "    return counter / len(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text_to_encode, encoding_dict, n_gram = 1):\n",
    "    splitted_to_grams = [text_to_encode[i:i + n_gram] for i in range(0, len(text_to_encode), n_gram)]\n",
    "    result = []\n",
    "    for gram in splitted_to_grams:\n",
    "        if(len(gram) < n_gram):\n",
    "            continue\n",
    "        result.append(encoding_dict[gram])\n",
    "    return \"\".join(result)\n",
    "\n",
    "\n",
    "def decode(encoded_text, decoding_dict, n_gram = 1):\n",
    "    splitted_to_grams = [encoded_text[i:i + n_gram] for i in range(0, len(encoded_text), n_gram)]\n",
    "    result = []\n",
    "    for gram in splitted_to_grams:\n",
    "        result.append(decoding_dict[gram])\n",
    "    return \"\".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.345679012345679\n"
     ]
    }
   ],
   "source": [
    "frequencies = get_frequncy_of_n_gram(ru_data)\n",
    "dict_for_encoding = get_encoding_dict(frequencies)\n",
    "encoded_text = encode(test, dict_for_encoding)\n",
    "\n",
    "frequencies_encoded = get_frequncy_of_n_gram(encoded_text)\n",
    "dict_for_decoding = get_decoding_dict(frequencies_encoded, frequencies)\n",
    "decoded_text = decode(encoded_text, dict_for_decoding)\n",
    "print(f\"Accuracy = {calculate_accuracy(test, decoded_text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Вряд ли в результате получилась такая уж хорошая расшифровка, разве что если вы брали в качестве тестовых данных целые рассказы. Но и Шерлок Холмс был не так уж прост: после буквы E, которая действительно выделяется частотой, дальше он анализировал уже конкретные слова и пытался угадать, какими они могли бы быть. Я не знаю, как запрограммировать такой интуитивный анализ, так что давайте просто сделаем следующий логический шаг:\n",
    "    - подсчитайте частоты биграмм (т.е. пар последовательных букв) по корпусам;\n",
    "    - проведите тестирование аналогично п.1, но при помощи биграмм.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.10082304526748971\n"
     ]
    }
   ],
   "source": [
    "frequencies = get_frequncy_of_n_gram(ru_data, 2)\n",
    "dict_for_encoding = get_encoding_dict(frequencies)\n",
    "encoded_text = encode(test, dict_for_encoding, 2)\n",
    "\n",
    "frequencies_encoded = get_frequncy_of_n_gram(encoded_text, 2)\n",
    "dict_for_decoding = get_decoding_dict(frequencies_encoded, frequencies)\n",
    "decoded_text = decode(encoded_text, dict_for_decoding, 2)\n",
    "print(f\"Accuracy = {calculate_accuracy(test, decoded_text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, с биграммами стало даже хуже"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Но и это ещё не всё: биграммы скорее всего тоже далеко не всегда работают. Основная часть задания — в том, как можно их улучшить:\n",
    "    - предложите метод обучения перестановки символов в этом задании, основанный на MCMC-сэмплировании, но по-прежнему работающий на основе статистики биграмм;\n",
    "    - реализуйте и протестируйте его, убедитесь, что результаты улучшились.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предлагаю использовать алгоритм Метрополиса-Гастингса. Будем переставлять случайные значения в словаре для декодирования и оценивать правдоподобие как произведение всех вероятностей биграм текста. Если правдоподобие выше для такой перестановки - сохраняем ее, если ниже, то сохраняем с вероятностью отношения нового и старого правдоподобий.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_log_likelihood(encoded_text, dict_for_decoding, frequncies_n_gram, n_gram):\n",
    "    decoded_text = decode(encoded_text, dict_for_decoding)\n",
    "    log_likelihood = 0\n",
    "    for i, _ in enumerate(decoded_text[:-1]):\n",
    "        bigram = decoded_text[i: i + n_gram]\n",
    "        probability = frequncies_n_gram.get(bigram, 1 / len(frequncies_n_gram))\n",
    "        log_likelihood += np.log(probability)\n",
    "    return log_likelihood\n",
    "\n",
    "def metropolis_hastings_log_accept(current_likelihood, new_likelyhood):\n",
    "    if new_likelyhood > current_likelihood:\n",
    "        return True\n",
    "    else:\n",
    "        return np.random.rand() < np.exp(new_likelyhood - current_likelihood)  \n",
    "\n",
    "def random_permutation(dict_for_decoding):\n",
    "    new_values = list(dict_for_decoding.values())\n",
    "    key_1, key_2 = np.random.choice(range(len(list(dict_for_decoding.values()))), 2, replace=False)\n",
    "    new_values[key_1], new_values[key_2] = new_values[key_2], new_values[key_1]\n",
    "    new_dict_for_decoding = {key: val for key, val in zip(list(dict_for_decoding.keys()), new_values)}\n",
    "    return new_dict_for_decoding\n",
    "\n",
    "def metropolis_hastings(encoded_text, frequencies_bigram, dict_for_decoding, iterations, n_gram):\n",
    "    current_dict_for_decoding = dict_for_decoding\n",
    "    best_dict_for_decoding = dict_for_decoding\n",
    "    best_likelihood = -np.inf\n",
    "    current_likelihood = func_log_likelihood(encoded_text, dict_for_decoding, frequencies_bigram, n_gram)\n",
    "  \n",
    "    for i in range(iterations):\n",
    "        new_dict_for_decoding = random_permutation(current_dict_for_decoding)\n",
    "        new_likelyhood = func_log_likelihood(encoded_text, new_dict_for_decoding, frequencies_bigram, n_gram)\n",
    "        if metropolis_hastings_log_accept(current_likelihood, new_likelyhood):\n",
    "            current_dict_for_decoding = new_dict_for_decoding\n",
    "            current_likelihood = new_likelyhood            \n",
    "        if current_likelihood > best_likelihood:\n",
    "            best_likelihood = current_likelihood\n",
    "            best_dict_for_decoding = current_dict_for_decoding\n",
    "    return best_likelihood, best_dict_for_decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies = get_frequncy_of_n_gram(ru_data)\n",
    "dict_for_encoding = get_encoding_dict(frequencies)\n",
    "encoded_text = encode(test, dict_for_encoding)\n",
    "\n",
    "frequencies_encoded = get_frequncy_of_n_gram(encoded_text)\n",
    "dict_for_decoding = get_decoding_dict(frequencies_encoded, frequencies)\n",
    "frequncies_bigram = get_frequncy_of_n_gram(ru_data, 2)\n",
    "\n",
    "likelihood, dict_for_decoding = metropolis_hastings(encoded_text, frequncies_bigram, dict_for_decoding, 30000, n_gram=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_text = decode(encoded_text, dict_for_decoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2655.7698035079948\n",
      "ак не говорите мне про австрий я ничего не понимай может быть но австрия нихогда не котела и не кочет вошны она предает нас россия одна должна быть спасительницеш европы наю благодетель знает свое высохое призвание и будет верен ему вот одно во что я верй наюему доброму и чудному государй предстоит величашюая роль в мире и он тах добродетелен и корою что бог не оставит его и он исполнит свое призвание задавить гидру револйции хоторая теперь еще ужаснее в лице этого убишцы и злодея \n",
      "Accuracy = 0.9567901234567902\n"
     ]
    }
   ],
   "source": [
    "print(likelihood)\n",
    "print(decoded_text)\n",
    "print(f\"Accuracy = {calculate_accuracy(test, decoded_text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Расшифруйте сообщение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = '←⇠⇒↟↹↷⇊↹↷↟↤↟↨←↹↝⇛⇯↳⇴⇒⇈↝⇊↾↹↟⇒↟↹⇷⇛⇞↨↟↹↝⇛⇯↳⇴⇒⇈↝⇊↾↹↨←⇌⇠↨↹⇙↹⇸↨⇛↙⇛↹⇠⇛⇛↲⇆←↝↟↞↹⇌⇛↨⇛⇯⇊↾↹⇒←↙⇌⇛↹⇷⇯⇛⇞↟↨⇴↨⇈↹⇠⇌⇛⇯←←↹↷⇠←↙⇛↹↷⇊↹↷⇠←↹⇠↤←⇒⇴⇒↟↹⇷⇯⇴↷↟⇒⇈↝⇛↹↟↹⇷⇛⇒⇙⇞↟↨←↹↳⇴⇌⇠↟↳⇴⇒⇈↝⇊↾↹↲⇴⇒⇒↹⇰⇴↹⇷⇛⇠⇒←↤↝←←↹⇞←↨↷←⇯↨⇛←↹⇰⇴↤⇴↝↟←↹⇌⇙⇯⇠⇴↹↘⇛↨↞↹⇌⇛↝←⇞↝⇛↹↞↹↝↟⇞←↙⇛↹↝←↹⇛↲←⇆⇴⇏'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies = get_frequncy_of_n_gram(ru_data)\n",
    "frequencies_encoded = get_frequncy_of_n_gram(message)\n",
    "dict_for_decoding = get_decoding_dict(frequencies_encoded, frequencies)\n",
    "frequncies_bigram = get_frequncy_of_n_gram(ru_data, 2)\n",
    "\n",
    "likelihood, dict_for_decoding = metropolis_hastings(message, frequncies_bigram, dict_for_decoding, 30000, n_gram = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihood = -1253.8571388105913\n",
      "если вы вимите норзальный или подти норзальный текст у чтого соожшения который легко продитать скорее всего вы все смелали правильно и полудите заксизальный жалл ба послемнее детвертое бамание курса хотя конедно я нидего не ожешаю\n"
     ]
    }
   ],
   "source": [
    "decoded_text = decode(message, dict_for_decoding)\n",
    "print(f\"Likelihood = {likelihood}\")\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Несмотря на мелкие ошибки, текст легко читается"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Бонус: а что если от биграмм перейти к триграммам (тройкам букв) или даже больше? Улучшатся ли результаты? Когда улучшатся, а когда нет? Чтобы ответить на этот вопрос эмпирически, уже может понадобиться погенерировать много тестовых перестановок и последить за метриками, глазами может быть и не видно.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies = get_frequncy_of_n_gram(ru_data)\n",
    "dict_for_encoding = get_encoding_dict(frequencies)\n",
    "encoded_text = encode(test, dict_for_encoding)\n",
    "\n",
    "frequencies_encoded = get_frequncy_of_n_gram(encoded_text)\n",
    "dict_for_decoding = get_decoding_dict(frequencies_encoded, frequencies)\n",
    "frequncies_trigram = get_frequncy_of_n_gram(ru_data, 3)\n",
    "\n",
    "likelihood, dict_for_decoding = metropolis_hastings(encoded_text, frequncies_trigram, dict_for_decoding, 30000, n_gram = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihood = -3646.6043511448934\n",
      "ах не говорите мне про австрий я ничего не понимай может быть но австрия никогда не хотела и не хочет вошны она предает нас россия одна должна быть спасительницеш европы наю благодетель знает свое высокое призвание и будет верен ему вот одно во что я верй наюему доброму и чудному государй предстоит величашюая роль в мире и он так добродетелен и хорою что бог не оставит его и он исполнит свое призвание задавить гидру револйции которая теперь еще ужаснее в лице этого убишцы и злодея \n",
      "0.9732510288065843\n"
     ]
    }
   ],
   "source": [
    "decoded_text = decode(encoded_text, dict_for_decoding)\n",
    "print(f\"Likelihood = {likelihood}\")\n",
    "print(decoded_text)\n",
    "print(calculate_accuracy(test, decoded_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies = get_frequncy_of_n_gram(ru_data)\n",
    "dict_for_encoding = get_encoding_dict(frequencies)\n",
    "encoded_text = encode(test, dict_for_encoding)\n",
    "\n",
    "frequencies_encoded = get_frequncy_of_n_gram(encoded_text)\n",
    "dict_for_decoding = get_decoding_dict(frequencies_encoded, frequencies)\n",
    "frequncies_4gram = get_frequncy_of_n_gram(ru_data, 4)\n",
    "\n",
    "likelihood, dict_for_decoding = metropolis_hastings(encoded_text, frequncies_4gram, dict_for_decoding, 30000, n_gram = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihood = -4448.792710139201\n",
      "ах не говорите мне про австрий я ничего не понимай может быть но австрия никогда не хотела и не хочет воцны она предает нас россия одна должна быть спасительниюец европы наш благодетель знает свое высокое призвание и будет верен ему вот одно во что я верй нашему доброму и чудному государй предстоит величацшая роль в мире и он так добродетелен и хорош что бог не оставит его и он исполнит свое призвание задавить гидру револйюии которая теперь еще ужаснее в лиюе этого убицюы и злодея \n",
      "0.9732510288065843\n"
     ]
    }
   ],
   "source": [
    "decoded_text = decode(encoded_text, dict_for_decoding)\n",
    "print(f\"Likelihood = {likelihood}\")\n",
    "print(decoded_text)\n",
    "print(calculate_accuracy(test, decoded_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кажется, что с ростом длины n-граммы повышается вероятность не найти подходящую перестановку и в итоге не найти хороший максимум правдоподобия, хотя, конечная точность становится выше"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Бонус: какие вы можете придумать применения для этой модели? Пляшущие человечки ведь не так часто встречаются в жизни (хотя встречаются! и это самое потрясающее во всей этой истории, но об этом я расскажу потом)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме как пытаться востановить кодировку мне ничего в голову не приходит :)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fd2d63775d6b861368f2b68b88b1b40a35d08c6b96cd47d9eb8c9533c1d8ad02"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
